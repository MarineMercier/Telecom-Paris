{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Séparateurs à Vaste Marge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_breastcancer(filename):\n",
    "    \"\"\"\n",
    "    Cette fonction lit le fichier filename, par exemple\n",
    "    filename = 'wdbc_M1_B0.data'\n",
    "    Elle retourne \n",
    "    X : une matrice de caracteristiques\n",
    "    y : un vecteur des classes tel que si y[i] = 1, la tumeur est maligne\n",
    "        et si y[i] = -1, la tumeur est benigne\n",
    "\n",
    "    Pour plus d'infos sur la base de donnees,\n",
    "    https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Prognostic%29\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "\n",
    "    # la colonne 0 ne nous interesse pas ici\n",
    "    y = data[:, 1] * 2 - 1\n",
    "    X = data[:, 2:]\n",
    "\n",
    "    # Standardisation de la matrice\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    X = X / np.std(X, axis=0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = load_breastcancer(\"wdbcM1B0.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "Le problème (1) est le suivant :\n",
    "$$ min_{v \\in \\mathbb{R}, a \\in \\mathbb{R}^{m}, \\zeta \\in \\mathbb{R}^{n}} \\frac{1}{2} \\sum_{j=1}^{m} V_{j}^{2} + C\\sum_{i=1}^{n} \\zeta_{i} $$ \n",
    "Avec pour tout i$\\in[\\![1,n ]\\!],$  on a 0$\\leqslant\\zeta_{i}$ et $1-y_{i}(x_{i}^{T}v+a)\\leqslant\\zeta_{i}$\n",
    "\n",
    "On nomme A le minimum du problème (1).\n",
    "\n",
    "\n",
    "Le problème (2) est le suivant : \n",
    "$$ min_{v \\in \\mathbb{R}, a \\in \\mathbb{R}^{m}} \\frac{1}{2} \\sum_{j=1}^{m} V_{j}^{2} + C\\sum_{i=1}^{n} max(0,1-y_{i}(x_{i}^{T}v+a)) $$ \n",
    "On appelle B le minimum du problème (2).\n",
    "\n",
    "\n",
    "En choisissant $\\zeta$ tel que pour tout $i\\in[\\![1,n ]\\!],\\zeta_{i}=max(0,1-y_{i}(x_{i}^{T}v+a))$, on a A $\\leqslant$ B, par définition de A et du minimum. \n",
    "\n",
    "De plus, pour tout i$\\in[\\![1,n ]\\!],$  on a 0$\\leqslant\\zeta_{i}$ et $1-y_{i}(x_{i}^{T}v+a)\\leqslant\\zeta_{i}$. \n",
    "Donc pour tout i$\\in[\\![1,n ]\\!],$  on a $max(0,1-y_{i}(x_{i}^{T}v+a)) \\leqslant\\zeta_{i}$.\n",
    "On obtient bien B $\\leqslant$ A.\n",
    "\n",
    "Les deux problèmes sont donc équivalents, car leur solutions sont égales. L'optimum de l'un est égal à l'optimum de l'autre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_=np.linspace(-4,6,11)\n",
    "Y_=[max(0,1-i)for i in range(-4,7)]\n",
    "plt.plot(X_,Y_,label=\"h(z)\")\n",
    "plt.plot([0,2], [-0.02,-.02], color = 'orange', label = \"$\\partial h(z)$ for z=1\")\n",
    "plt.plot([0,2], [0.98, -1.02], color = 'orange')\n",
    "plt.plot([0,2], [0.5, -.5], color = 'orange')\n",
    "plt.plot([0,2], [0.3,-0.3], color = 'orange')\n",
    "plt.plot([-4,-1],[4.9,1.9],color='red',label='$\\partial h(z)$ for z<1')\n",
    "plt.plot([3,6],[-0.05,-0.05],color='pink',label='$\\partial h(z)$ for z>1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe graphiquement que le subgradient de h est égal à $[-1,0]$ si z=1.\n",
    "Comme h est dérivable sauf en 1, $\\frac{dh}{dz} (z) = \\partial h(z)$. \n",
    "\n",
    "Ainsi si z<1, on a $\\partial h(z)$={-1}.\n",
    "Et pour z>1, on a $\\partial h(z)$={0}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3\n",
    "\n",
    "On pose N(v,a)=$\\frac{1}{2} \\sum_{j=1}^{m} v_{j}^{2}$. \n",
    "\n",
    "N:$\\mathbb{R}^{m+1}\\mapsto\\mathbb{R}$\n",
    "\n",
    "Pour tout i$\\in[\\![1,m]\\!],n_{j}(v_{j})=\\frac{1}{2}v_{j}^{2}$, on a bien N(v,a)=$ \\sum_{j=1}^{m} n_{j}(v_{j})$. \n",
    "\n",
    "N est donc séparable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pose H:z$\\mapsto \\sum_{i=1}^{n} max(0,1-z_{i})$. \n",
    "\n",
    "H: $\\mathbb{R}^{n}\\mapsto\\mathbb{R}$\n",
    "          \n",
    "Pour tout i$\\in[\\![1,m]\\!],h_{j}(z_{i})=max(0,1-z_{i})$, on a bien H=$ \\sum_{i=1}^{n} h_{i}(z_{i})$. \n",
    "\n",
    "Donc H est séparable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pose M:v,a$\\mapsto y_{i}(x_{i}^{T}v+a)$.\n",
    "\n",
    "M:$\\mathbb{R}^{m+1}\\mapsto\\mathbb{R}^{n}$\n",
    "\n",
    "Soit l$\\in \\mathbb{R}$, v,w$\\in \\mathbb{R} ^{2}$. \n",
    "$M(v+l*w,a)=y_{i}(x_{i}^{T}(v+l*w)+a)=y_{i}(x_{i}^{T}v+a)+l*y_{i}(x_{i}^{T}w+a)=M(v,a)+l*M(w,a)$.\n",
    "\n",
    "De même, M est linéaire selon a par linéarité du produit matriciel. \n",
    "\n",
    "Ainsi M est linéaire. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par définition de M,H et N : $$f(v,a)=N(v,a)+cH(M(v,a))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom N = $\\mathbb{R}^{m+1}$, dom H=$\\mathbb{R}^{n}$, comme M est linéaire et 0 $\\in$ dom H, \n",
    "on a  0$\\in$ relint(dom H - $M^{T}$ dom N). H et N sont convexes . \n",
    "\n",
    "On peut donc appliquer la proposition 2.4.2 :\n",
    "$$\\partial f(v,a)=\\partial(N(v,a)+H(M(v,a)))=\\partial N(v,a)+cM^{T}\\partial H(M(v,a))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\partial N  (v,a)$ =  { $\\triangledown N(v,a) $} = {$\\begin{bmatrix} \\frac{\\partial N  (v,a)}{\\partial v} \\\\ \\frac{\\partial N  (v,a)}{\\partial a}\\end{bmatrix}$}={$\\begin{bmatrix} v \\\\ 0 \\end{bmatrix}$} car N est differentiable selon chacune de ses composante ($v_{1},v_{2},...,v_{m},a)$).\n",
    "\n",
    "$\\partial H (Z) $={$\\begin{bmatrix} -1 si Z_{i}<1  \\\\ [-1,0] si Z_{i}=1 \\\\ 0 sinon \\end{bmatrix}$} car H est séparable, et d'après la question 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M = np.dot(np.diag(y), np.concatenate([X, np.ones((569,1))],axis=1))\n",
    "\n",
    "def N(v,a):\n",
    "    return np.sum(v**2)/2\n",
    "\n",
    "def H(z):\n",
    "    return np.sum(np.maximum(np.zeros(z.shape),1-z))\n",
    "\n",
    "def f(v,a):\n",
    "    c=1\n",
    "    f=N(v,a) + c * H(np.dot(M,np.concatenate([v, [a]])))\n",
    "    return f\n",
    "\n",
    "def subgrad_N(v,a):\n",
    "    return np.concatenate([v, [0]])\n",
    "\n",
    "def subgrad_H(Z):\n",
    "    return  (Z<=1)*(-1) #on choisit le subgradient -1 si z==1\n",
    "\n",
    "def subgrad_f(v,a):\n",
    "    c=1\n",
    "    return subgrad_N(v,a) + c * np.dot(np.transpose(M),subgrad_H(np.dot(M,np.concatenate([v, [a]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def methode_sous_gradient(v,a):\n",
    "    X=np.concatenate([v, [a]])\n",
    "    res= math.inf\n",
    "    k=0\n",
    "    while res>0.01 :\n",
    "        gamma = 1/np.sqrt(k+1)\n",
    "        Xmemory=X.copy()\n",
    "        X = X - gamma*subgrad_f(X[:-1],X[-1]) \n",
    "        res=np.abs(f(Xmemory[:-1],Xmemory[-1])-f(X[:-1],X[-1]) )\n",
    "        k+=1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Situation initiale : f(v0,a0)=569.0\n",
      "Situation finale après optimisation : f(v*,a*)=48.08520013665914\n"
     ]
    }
   ],
   "source": [
    "v = np.zeros((30,))\n",
    "a=0\n",
    "print(\"Situation initiale : f(v0,a0)=\"+str(f(v,a)))\n",
    "Xsol = methode_sous_gradient(v,a)\n",
    "print(\"Situation finale après optimisation : f(v*,a*)=\"+str(f(Xsol[:-1],Xsol[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E(f_I(v,a)) = \\sum_i P(I = i) \\times f_i(v,a)  = \\frac{1}{n}\\sum_i f_i(v,a) = \\frac{1}{2} \\sum_j v_j^2 + \\frac{1}{n}\\sum_i c\\,n\\,max(0,1-y_i(x_i^Tv + a))\\\\ E(f_I(v,a)) = \\frac{1}{2} \\sum_j v_j^2 + c\\sum_i max(0,1-y_i(x_i^Tv + a))$\n",
    "\n",
    "Donc $ E(f_I(v,a)) = f(v,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a $f_i(v,a)=\\frac{1}{2} \\sum_j v_j^2 + c\\,n\\,max(0,1-y_i(x_i^Tv + a))=N(v,a)+H(Mi(v,a)^T)$\n",
    "En posant $M=y(x^T,1)$ et Mi la ième colone de M.\n",
    "\n",
    "Ainsi : $$\\partial f(v,a)=\\partial(N(v,a)+H(Mi(v,a)))=\\partial N(v,a)+cnMi^{T}\\partial H(Mi(v,a))$$\n",
    "\n",
    "$\\partial f_i(v,a) = \\left \\{ \\begin{array} {} \\{(v,0) - ncM_i^T\\} & if \\, M_i(v,a) \\lt 1 \\\\ \\{(v,0) + ncM_i^T,\\,t \\in [-1;0]\\} & if \\, M_i(v,a) = 1 \\\\ \\{(v,0)\\} & if \\, M_i(v,a) \\gt 1 \\\\ \\end{array} \\right .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(M)\n",
    "\n",
    "def subgrad_H_M_i(v,a, i):\n",
    "    va=np.concatenate([v, [a]])\n",
    "    return ((np.dot(M[i,:],va)>=1)- 1) * M[i,:]\n",
    "\n",
    "def subgrad_f_i(v,a, i):\n",
    "    va=np.concatenate([v, [a]])\n",
    "    return subgrad_N(v,a) + c * M.shape[0] * subgrad_H_M_i(v,a,i)\n",
    "\n",
    "def methode_sous_gradient_stochastique(v,a):\n",
    "    X=np.concatenate([v, [a]])\n",
    "    res= math.inf\n",
    "    k=0\n",
    "    X_moy = np.zeros(X.shape)\n",
    "    gamma_sum = 0\n",
    "    while res>0.001:\n",
    "        I=np.random.randint(n)\n",
    "        gamma = 1/np.sqrt(k+1)\n",
    "        Xmemory=X.copy()\n",
    "        X_moy+=X*gamma\n",
    "        gamma_sum+=gamma\n",
    "        X = X - gamma*subgrad_f_i(X[:-1],X[-1],I) \n",
    "        res=np.abs(f(Xmemory[:-1],Xmemory[-1])-f(X[:-1],X[-1]) )\n",
    "        k+=1\n",
    "    return X_moy/gamma_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Situation initiale : f(v0,a0)=569.0\n",
      "Situation finale après optimisation avec gradient stochastique: f(v*,a*)=275.5491334272896\n"
     ]
    }
   ],
   "source": [
    "v = np.zeros((30,))\n",
    "c=1\n",
    "print(\"Situation initiale : f(v0,a0)=\"+str(f(v,a)))\n",
    "Xsol = methode_sous_gradient_stochastique(v,a)\n",
    "print(\"Situation finale après optimisation avec gradient stochastique: f(v*,a*)=\"+str(f(Xsol[:-1],Xsol[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats avec cette méthode d'optimisation sont très variables : entre 30 et 300. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le lagrangien du problème (1) s'écrit :\n",
    " $$L(v,a,\\zeta,\\phi)=\\sum_{j=1}^{m} V_{j}^{2} + C\\sum_{i=1}^{n} \\zeta_{i}+\\sum_{i=1}^{n} \\phi_{i}(1-y_{i}(x_{i}^Tv+a)-\\zeta_{i})-\\sum_{i=1}^{n} \\phi_{i}\\zeta_{i}+\\sum_{i=1}^{n} l_{\\mathbb{R}^{+}}(\\phi_{i})$$\n",
    " $$L(v,a,\\zeta,\\phi)=\\sum_{j=1}^{m} V_{j}^{2} + C\\sum_{i=1}^{n} \\zeta_{i}+\\sum_{i=1}^{n} \\phi_{i}(1-y_{i}(x_{i}^Tv+a)-2\\zeta_{i})+\\sum_{i=1}^{n} l_{\\mathbb{R}^{+}}(\\phi_{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerons d'abord g comme une fonction de x pour tout $\\phi$ :\n",
    "$$\\forall x>-\\frac{\\phi}{\\rho}, g(x,\\phi)=\\frac{x^2 \\rho}{2}+x\\phi$$ \n",
    "Sa dérivée $\\forall x>-\\frac{\\phi}{\\rho}$ vaut $\\nabla_{x}g(x,\\phi)=\\phi+x\\rho>0$. Ce résultat tend vers 0 quand x tend vers $-\\frac{\\phi}{\\rho}$.\n",
    "Au contraire, $\\forall x<-\\frac{\\phi}{\\rho},g(x,\\phi)=-\\frac{\\phi^{2}}{2\\rho}$, donc $\\nabla_{x}g(x,\\phi)=0$.\n",
    "Donc g est dérivable selon x sur R, et $\\nabla_xg(x, \\phi) = \\left \\{ \\begin{array} {} 0 & if \\, x \\le - \\frac{\\phi}{\\rho} \\\\ \\rho x + \\phi & if \\, x \\gt - \\frac{\\phi}{\\rho} \\\\ \\end{array} \\right . = \\rho \\max (0, x+\\frac{\\phi}{\\rho})$\n",
    "\n",
    "On procède de même en considérant g comme une fonction de  $\\phi$ pour tout x:  $\\nabla_\\phi g(x, \\phi) = \\left \\{ \\begin{array} {} -\\frac{\\phi}{\\rho} & if \\, \\phi \\le - x{\\rho} \\\\ x & if \\, \\phi \\gt - x{\\rho} \\\\ \\end{array} \\right . =\\max (-\\frac{\\phi}{\\rho}, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\forall x \\in \\mathbb{R} \\nabla_xg(x, \\phi)>0$, donc g(.,$\\phi$) est convexe.\n",
    "De même, $\\nabla_{\\phi}g(x, \\phi)$ est décroissante pour x fixé, ainsi, g(x,.) est concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 2\n",
    "\n",
    "def f_bis(v,xi):\n",
    "    return 1/2 * np.sum(v**2) + c * np.sum(xi)\n",
    "\n",
    "def subgrad_g_x(x, phi):\n",
    "    return rho*np.maximum(np.zeros(x.shape),x+phi/rho)\n",
    "\n",
    "def subgrad_g_phi(x, phi):\n",
    "    return np.maximum(-phi/rho,x)\n",
    "\n",
    "def descente_grad_L(v0,a0,xi0,phi0,psi0):\n",
    "    v = v0.copy()\n",
    "    a = a0\n",
    "    xi = xi0.copy()\n",
    "    phi=phi0\n",
    "    psi=psi0\n",
    "    d_v = np.zeros((30,))\n",
    "    d_a = 1.5\n",
    "    d_xi = np.zeros((569,))\n",
    "    while np.sum(d_v**2)+d_a**2+np.sum(d_xi**2)>1:\n",
    "        gamma = 10**(-4)\n",
    "        d_v =( v - np.dot(np.dot(np.diag(y), X).T, subgrad_g_x(1-xi-np.dot(np.diag(y), np.dot(X, v) + a), psi)))\n",
    "        d_a=np.sum(np.dot(-np.diag(y),subgrad_g_x(1-xi-np.dot(np.diag(y), np.dot(X, v) + a),psi)))\n",
    "        d_xi =(c - subgrad_g_x(-xi, phi) - subgrad_g_x(1-xi-np.dot(np.diag(y), np.dot(X, v) + a), psi))\n",
    "        a-=d_a*gamma\n",
    "        v-=d_v*gamma\n",
    "        xi-=d_xi*gamma\n",
    "    return v, a, xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Situation initiale : f(v0,a0)=0.0\n",
      "Situation finale après optimisation : f(v*,a*)=33.157988097341516\n"
     ]
    }
   ],
   "source": [
    "v0 = np.zeros((30,))\n",
    "a0 = 0\n",
    "xi0 = np.zeros((569,))\n",
    "phi0=np.ones((569,))\n",
    "psi0=np.ones((569,))\n",
    "print(\"Situation initiale : f(v0,a0)=\"+str(f_bis(v0,xi0)))\n",
    "v,a,xi = descente_grad_L(v0,a0,xi0,phi0,psi0)\n",
    "print(\"Situation finale après optimisation : f(v*,a*)=\"+str(f_bis(v,xi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgrad_L_phi(v,a,xi,phi,psi):\n",
    "    return subgrad_g_phi(-xi, phi)\n",
    "\n",
    "def subgrad_L_psi(v,a,xi,phi,psi):\n",
    "    x = 1-xi-np.dot(np.diag(y), np.dot(X, v) + a)\n",
    "    return subgrad_g_phi(1-xi-np.dot(np.diag(y), np.dot(X, v) + a), psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrangien_augmenté(N):\n",
    "    phi = 0\n",
    "    psi = 0\n",
    "    v0 = np.zeros((30,))\n",
    "    a0 = 0\n",
    "    xi0 = np.zeros((569,))\n",
    "    for i in range(N):\n",
    "        v,a,xi =descente_grad_L(v0,a0,xi0,phi,psi)\n",
    "        phi += rho * subgrad_g_phi(-xi, phi)\n",
    "        psi += rho * subgrad_g_phi(1-xi-np.dot(np.diag(y), np.dot(X, v) + a), psi)\n",
    "    return v,a,xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v,a,xi = lagrangien_augmenté(2000)\n",
    "print(\"Situation finale après optimisation : f(v*,a*)=\"+str(f_bis(v,xi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode du sous-gradient est plus efficace que celle du gradient stochastique. En effet la méthode du gradient stochastique renvoie des résultats très variables. Pour rendre cette méthode plus efficace il faurait augmenter grandement le nombre de données. \n",
    "\n",
    "La méthode du Lagrangien augmenté donne les meilleurs résultats mais est aussi très longue et demande beaucoup de calculs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
