{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHxS3VcZ86wa"
   },
   "source": [
    "# Practical work on SAR statistics - IMA207\n",
    "\n",
    "### Emanuele DALSASSO, Florence TUPIN\n",
    "\n",
    "Images of the practical work can be found on: \n",
    "https://perso.telecom-paristech.fr/dalsasso/TPSAR/\n",
    "and \n",
    "https://perso.telecom-paristech.fr/tupin/TPSAR/DATA/images/\n",
    "\n",
    "Some useful functions are available in the file *mvalab.py*. \n",
    "\n",
    "### Study of homogeneous areas\n",
    "\n",
    "You have at your disposal a set of images coming from different sensors and with different characteristics on the same area of Flevoland in Netherlands (for each sensor and acquisition mode, an homogeneous area of sea has been selected with *mer* extension, and an area of farmland with  *centre* extension):\n",
    "- Sentinel-1 sensor (ESA), SLC (Single look Complex) data and GRD (Ground Range Detected) data ;\n",
    "- ERS sensor (ESA), PRI product (ground range data);\n",
    "- Alos sensor (JAXA), SLC (Single look Complex) data.\n",
    "\n",
    "\n",
    "\n",
    "### First steps\n",
    "The first two cells will set the display at full screen and disable cell scrolling, allowing for a more pleasant display of the figures\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "Sf7dfPsZfQ7j",
    "outputId": "1b57e580-cb73-466d-8edb-689260fa78b3"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "T0Q4p4YgfQ8I",
    "outputId": "7f81492d-8401-48fa-f90e-116b261cef61"
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQxFfpSCfQ8Y"
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "# if using google colab:\n",
    "%matplotlib inline \n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import mvalab as mvalab\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "plt.rcParams['figure.max_open_warning'] = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "qE0-4mm-JSH7"
   },
   "source": [
    "## A. Single look data distributions \n",
    "In this part, we will use an SLC (Single Look Complex) image and analyze its distribution. \n",
    "The image has been acquired by the Sentinel-1 sensor over the Lelystad zone (very flat area with fields crops). \n",
    "Vizualize the amplitude image and interpret it. \n",
    "\n",
    "NB amplitude image is given by the modulus of the electro-magnetic field and intensity is the square of the amplitude (proportional to the signal power). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "hidden": true,
    "id": "JLSWCECQJjzm",
    "outputId": "5064e265-e136-4cb1-f2fa-29eda12d12ba"
   },
   "outputs": [],
   "source": [
    "import mvalab as mva\n",
    "pageweb=\"https://perso.telecom-paristech.fr/dalsasso/TPSAR/pilelely/\"\n",
    "image='Lely.CXF'\n",
    "im_slc_senti_lely_liste=mvalab.imz2mat(pageweb+image);\n",
    "im_slc_senti_lely = im_slc_senti_lely_liste[0]\n",
    "ncol=im_slc_senti_lely_liste[1]\n",
    "nlig=im_slc_senti_lely_liste[2]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "# plot the amplitude image\n",
    "mva.visusar(im_slc_senti_lely)\n",
    "#- insert code here to display the image\n",
    "# because the image is big you can display a 1024 x 1024 pixels part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "LpstbS21KD7v"
   },
   "source": [
    "### A.1. Data distributions for an homogeneous area\n",
    "Select a **physically homogeneous** area and compute the distribution of the real part, imaginary part, phase, intensity and amplitude. Some useful functions are:\n",
    "- `np.angle`\n",
    "- `np.real`\n",
    "- `np.imag` \n",
    "\n",
    "Compute the histograms for these variables and the corfficient of variation ($\\gamma=\\frac{\\sigma}{\\mu}$) in intensity and amplitude. \n",
    "\n",
    "NB The Goodman model is valid only on homogeneous area. That is why it is important to select pixels sharing the same distribution (with the same underlying reflectivity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "qQeBa4zVfQ8_"
   },
   "outputs": [],
   "source": [
    "# Select a crop of the image (around 200 by 200 pixels)\n",
    "crop_slc = im_slc_senti_lely[900:1000,0:200]\n",
    "mvalab.visusar(crop_slc)\n",
    "\n",
    "# Compute amplitude, intensity, phase, real and imaginary part\n",
    "amp_senti_lely = np.abs(crop_slc)\n",
    "int_senti_lely = np.square(amp_senti_lely)\n",
    "ph_senti_lely = np.angle(crop_slc)\n",
    "real_senti_lely = np.real(crop_slc)\n",
    "imag_senti_lely = np.imag(crop_slc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "rltrId_r86w9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the histograms and verify they match the theoretical distribution\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(amp_senti_lely.ravel(),bins='auto',density=True,range=[0.,100])  \n",
    "plt.title('histogram of amplitude')\n",
    "plt.show()  \n",
    "\n",
    "plt.figure()\n",
    "#compute histogram for intensity data \n",
    "plt.hist(int_senti_lely.ravel(),bins='auto',density=True,range=[0.,5000])\n",
    "plt.title('histogram of intensity')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "#compute histogram for phase data \n",
    "plt.hist(ph_senti_lely.ravel(),bins='auto',density=True,range=[-np.pi,np.pi])\n",
    "plt.title('histogram of phase')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "#compute histogram for real part \n",
    "plt.hist(real_senti_lely.ravel(),bins='auto',density=True,range=[-100,100])\n",
    "plt.title('histogram of real part')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "#compute histogram for imaginary part\n",
    "plt.hist(imag_senti_lely.ravel(),bins='auto',density=True,range=[-100,100])\n",
    "plt.title('histogram of imaginary part')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "EiTJeyKwfQ9a"
   },
   "outputs": [],
   "source": [
    "# Compute the coefficient of variation on the homogeneous crop in amplitude format\n",
    "\n",
    "m_A = np.mean(amp_senti_lely)\n",
    "sigma_A = np.std(amp_senti_lely)\n",
    "coeff_var_A = sigma_A/m_A\n",
    "print(\"Variance Amplitude \" + str(coeff_var_A))\n",
    "\n",
    "# Compute the coefficient of variation on the homogeneous crop in intensity format\n",
    "m_A = np.mean(int_senti_lely)\n",
    "sigma_A = np.std(int_senti_lely)\n",
    "coeff_var_A = sigma_A/m_A\n",
    "print(\"Variance Intensité \" + str(coeff_var_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpCilYL1poy4"
   },
   "source": [
    "## Question 1\n",
    "What are the distributions followed by real part, imaginary part, phase, intensity and amplitude on an homogeneous area ? \n",
    "What are the values for the coefficient of variation for amplitude and intensity data? Are they in accordance with the theoretical values ? \n",
    "\n",
    "---\n",
    "__Réponse:__\n",
    "\n",
    "L'amplitude suit une loi de Rayleigh. \n",
    "L'intensité suit une loi exponentielle décroissante.\n",
    "La phase suit une loi uniforme sur [-pi,pi].\n",
    "Les parties réelles et imaginaires suivent des lois gaussiennes. \n",
    "\n",
    "Les coefficients de variations sont 0.54 pour l'amplitude et 1.05 pour l'intensité, très proches des valeurs théoriques (0.523 et 1). LA différence peut s'expliquer car les hypothès ne sont pas totalement respectées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEKsB2MQ86xA"
   },
   "source": [
    "## B. Multi-looking of data\n",
    "A common way to reduce the speckle is to multi-look the data.\n",
    "\n",
    "Compute a multi-look down-sampled image of Lely SLC data `im_slc_senti_lely` using a factor of 1 in the vertical direction and 4 in horizontal (this will give almost square pixels). You can do so by convolving the image with an average window with the aforementioned dimensions. Use `signal.convolve2d` with `mode = 'same'`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwB7g_S-86xB"
   },
   "outputs": [],
   "source": [
    "# Create the mask that will be used to average pixels within the horizontal window of dimension 4\n",
    "masque_vert = np.ones((1,4))/4\n",
    "\n",
    "# Compute multilooking in intensity by convolving the entire image with the window just created\n",
    "sentinel_ml_int =signal.convolve2d(np.multiply(im_slc_senti_lely,np.conj(im_slc_senti_lely)),masque_vert,mode='same')\n",
    "\n",
    "# Downsample the image by a factor of 4, so to have squared pixels, and plot it in amplitude \n",
    "mvalab.visusar(np.sqrt(sentinel_ml_int[:,::4]))\n",
    "plt.suptitle(u'SENTINEL : Multivue en intensité')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0I3I4MFfQ9x"
   },
   "outputs": [],
   "source": [
    "# Compute multilooking in amplitude\n",
    "sentinel_ml_amp = signal.convolve2d(np.abs(im_slc_senti_lely),masque_vert,mode='same')\n",
    "\n",
    "# Downsample the image by a factor of 4, so to have squared pixels, and plot it in amplitude \n",
    "mvalab.visusar(sentinel_ml_amp[:,::4]) \n",
    "plt.suptitle(u'SENTINEL : Multivue en amplitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vuo--0pXfQ93"
   },
   "outputs": [],
   "source": [
    "# Compute the ratio between the multilooked intensity and amplitude to see the differences.\n",
    "# WARNING: prevent division by 0\n",
    "ratio = np.divide(np.abs(im_slc_senti_lely[:,::4]),np.sqrt(sentinel_ml_int[:,::4])+0.000001)\n",
    "ratio = np.abs(ratio).astype(np.float32)\n",
    "mvalab.visusar(ratio,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A2k-q8cxqZOz"
   },
   "source": [
    "## Question 2.\n",
    "Comment the effect of multi-looking. \n",
    "\n",
    "Multilooking can be computed both in intensity and in amplitude format. Can you see the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Résultats :__\n",
    "    Le multivue réduit la taille de l'image dans la direction horizontale, mais les pixels sont carrés. On observe moins de bruit, et les pixels sont isotropes ce qui rend l'image plus facilement interprétable.\n",
    "\n",
    "Les résultats se ressemblent beaucoup, on voit que la moyenne du ratio est environ de 1 et que l'écart type est très faible. \n",
    "\n",
    "Sur l'image du ratio on observe cependant quelques différences. La moyenne est autour de 1 avec de faibles variations, mais autour des points brillants, on observe des petites anomalies. Cela montre que les techniques de moyennage n'ont pas exactement le même effet autour des points très lumineux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "iGCkTGVb86xI"
   },
   "source": [
    "## C. Computation of the Equivalent Number of looks on homogeneous areas\n",
    "In this part you have at your disposal 3 images of a part of the sea. One is a Sentinel-1 GRD image and the other one is an ERS image. The multi-looking has been done by the data provider (ESA, European Space Agency).\n",
    "Use the value of the coefficient of variation to find the equivalent number of looks (ENL) of the Sentinel-1 GRD and ERS data ($\\gamma_I=\\frac{1}{\\sqrt{L}}$ for intensity data, $\\gamma_A=\\frac{0.523}{\\sqrt{L}}$ for amplitude). Normally the ENL should be an integer corresponding to the number of aberaged samples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "slTRUt8k86xJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Intensité\n",
    "pageweb='https://perso.telecom-paristech.fr/tupin/TPSAR/DATA/images/'\n",
    "image = 'SentinelGRD_flevoland_mer.imw'\n",
    "im_sentigrd_mer = mvalab.imz2mat(pageweb+image)\n",
    "mvalab.visusar(np.abs(im_sentigrd_mer[0]))\n",
    "\n",
    "\n",
    "# compute coefficient of variation and number of looks in amplitude\n",
    "amp_grd=np.abs(im_sentigrd_mer[0])\n",
    "m_grd= np.mean(amp_grd)\n",
    "sigma_grd = np.std(amp_grd)\n",
    "coeff_var_grd = sigma_grd/m_grd\n",
    "L_grd = np.square(0.523/coeff_var_grd)\n",
    "print('--- coeff var and L ---')\n",
    "print(coeff_var_grd)\n",
    "print(\"L calculé en amplitude \" + str(L_grd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_grd=amp_grd**2\n",
    "var_grd = np.std(int_grd.ravel())/np.mean(int_grd.ravel())\n",
    "L_grd = (1/var_grd)**2\n",
    "print('--- coeff var and L ---')\n",
    "print(coeff_var_grd)\n",
    "print(\"L calculé en intensité\",L_grd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "x2nQoRk9fQ-F"
   },
   "outputs": [],
   "source": [
    "image = 'ERS_flevoland_mer.imw'\n",
    "im_ers_mer = mvalab.imz2mat(pageweb+image)\n",
    "mvalab.visusar(np.abs(im_ers_mer[0]))\n",
    "\n",
    "# compute coefficient of variation and number of looks in amplitude\n",
    "amp_ers=np.abs(im_ers_mer[0])\n",
    "m_ers= np.mean(amp_ers)\n",
    "sigma_ers = np.std(amp_ers)\n",
    "coeff_var_ers = sigma_ers/m_ers\n",
    "L_ers = np.square(0.523/coeff_var_ers)\n",
    "\n",
    "print('--- coeff var and L ---')\n",
    "print(coeff_var_ers)\n",
    "print(\"L calculé en amplitude \" + str(L_ers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_ers=amp_ers**2\n",
    "var_ers = np.std(int_ers.ravel())/np.mean(int_ers.ravel())\n",
    "L_ers = (1/var_ers)**2 \n",
    "print('--- coeff var and L ---')\n",
    "print(coeff_var_ers)\n",
    "print(\"L avec l'intensité\",L_ers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9qbiIXWuuGK"
   },
   "source": [
    "## Question 3.\n",
    "Comment the number of looks you have found for Sentinel1-GRD and ERS data.\n",
    "\n",
    "---\n",
    "__Réponse :__\n",
    "\n",
    "Pour Sentinel1-GRD, on obtient environ les 5 vues réelles attendu, car on trouve 4.79 en amplitude.\n",
    "\n",
    "Pour ERS, on obtient 2.69 en amplitude, soit environ 3 vues réelles.\n",
    "\n",
    "Si l'estimation de L est bonne, on n'obtient pas de valeurs exactes car les conditions de Goodman ne sont pas respectées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "BApTwOUK86xL"
   },
   "source": [
    "## D. Local coefficient of variation\n",
    "\n",
    "The coefficient of variation $\\gamma=\\frac{\\sigma}{\\mu}$ (standard deviation normalized by the mean) is an indication of the local homogeneity of the scene. \n",
    "It can be computed locally around each pixel using a moving window.\n",
    "\n",
    "Using 2D convolution to speed up the processing, compute the images of coefficient of variation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "lzLnrpyU86xM"
   },
   "outputs": [],
   "source": [
    "# Select one of the following images: \n",
    "#'SentinelSLC_flevoland_centre.cxs', \n",
    "#'Alos_flevoland_centre.cxf',\n",
    "#'ERS_flevoland_centre.imw'\n",
    "\n",
    "image = 'SentinelSLC_flevoland_centre.cxs'\n",
    "ima_slc = mvalab.imz2mat(pageweb+image)\n",
    "\n",
    "# take the intensity \n",
    "ima_int = np.multiply(ima_slc[0], np.conj(ima_slc[0]))\n",
    "# crop of the image to ease display and computation\n",
    "ima_int=ima_int[0:512, 1024:2048]\n",
    "mvalab.visusar(np.sqrt(ima_int));\n",
    "\n",
    "\n",
    "# create the average window\n",
    "size_window = 7\n",
    "masque_loc = np.ones((size_window,size_window))/size_window**2\n",
    "\n",
    "# compute the image of the local means in intensity\n",
    "ima_int_mean = signal.convolve2d(ima_int,masque_loc,mode='same')\n",
    "\n",
    "# compute the image of the local variances (var{I} = E{I^2} - E{I}^2)\n",
    "ima_int_square = np.multiply(ima_int,ima_int) # I^2 \n",
    "ima_int_mean_square = signal.convolve2d(ima_int_square,masque_loc,mode='same')# E{I^2}\n",
    "ima_variance = ima_int_mean_square - np.multiply(ima_int_mean,ima_int_mean) \n",
    "\n",
    "# compute the image of the local coefficient of variation\n",
    "ima_coeff_var = np.divide(np.sqrt(ima_variance),ima_int_mean)\n",
    "mvalab.visusar(ima_coeff_var,6)\n",
    "plt.suptitle(u'Coefficient of variation')\n",
    "\n",
    "# plot the standard deviation: comment the result\n",
    "mvalab.visusar(np.sqrt(ima_variance),2)\n",
    "plt.suptitle(u'Standard deviation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hh9dLQosvUv9"
   },
   "source": [
    "## Question 4. \n",
    "Comment the results of the image of local coefficient of variation and local standard deviation. Which structures of the image are highlighted with the coefficient of variation ? What is the influence of the window size ? \n",
    "Why the local standard deviation is not adapted to measure the local homogeneity of the scene ?\n",
    "\n",
    "---\n",
    "__Réponse :__\n",
    "\n",
    "L'image du coefficient de variation est un détecteur de contours : sa valeur est forte au niveau des cibles très brillantes et des contours entre un champ clair et un champ sombre, mais la valeur de l'image est faible au niveau des zones homogènes telles que l'intérieur des champs.\n",
    "\n",
    "La fenêtre détermine la taille sur laquelle on va moyenner l'image, soit la taille des pixels de la nouvelle image.\n",
    "Un masque trop grand floute beaucoup l'image, on perd en détail et en information. On risque de ne pas bien détecter les contours si la taille du masque est plus grande que certain champs.\n",
    "Si le masque trop petit, l'image de la variance est très bruitée. Il faut prendre une fenêtre suffisament grande pour détecter les changements de couleurs.\n",
    "\n",
    "L'écart-type local detecte bien les points lumineux, mais c'est un mauvais détecteur de l'hétérogénéité.\n",
    "En effet, les champs clairs sont considérés comme hétérogènes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "nRmWP7uF86xP"
   },
   "source": [
    "## E. Image despeckling: simple averaging and Lee filter\n",
    "\n",
    "The local coefficient of variation is also used in a very famous filter for SAR images: the Lee filter. \n",
    "The principle of the filter is to combine the pixel value $I_s$ (intensity value of pixel $s$) and the local mean $\\hat{\\mu}_{s}$ depending on the local coefficient of variation $\\hat{\\gamma}_s$ with the following formula :\n",
    "$\n",
    "  \\hat{I}_s= \\hat{\\mu}_{s}+k_s (I_s-\\hat{\\mu}_{s})$\n",
    "\n",
    "and\n",
    "$\n",
    "  k_s=1- \\frac{\\gamma_{Sp}^2}{\\hat{\\gamma}^2_s}\n",
    "$\n",
    "\n",
    "$\\gamma_{Sp}$ is the theoretical value of the coefficient of variation for a pure speckle ($\\gamma_{Sp}=\\frac{1}{\\sqrt{L}}$ for a L-look intensity image).\n",
    "\n",
    "Using the previous map, compute the resulting image with Lee filter. \n",
    "\n",
    "Warning : $k$ should be in $[0,1]$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "kZ5VSHhL86xQ"
   },
   "outputs": [],
   "source": [
    "# compute the image of ks coefficients, by taking ima_coeff_var previously computed\n",
    "ks = 1-(1/(ima_coeff_var*ima_coeff_var))\n",
    "\n",
    "# force ks to have values comprised in the range [0,1]\n",
    "ks[ks<0]=0\n",
    "mvalab.visusar(ks,0)\n",
    "\n",
    "# filter the image\n",
    "A=np.multiply(ks,ima_int-ima_int_mean)\n",
    "image_lee_filtered = ima_int_mean+A\n",
    "mvalab.visusar(np.sqrt(image_lee_filtered))\n",
    "plt.suptitle(u'Image denoised using Lee filter')\n",
    "mvalab.visusar(np.sqrt(ima_int))\n",
    "plt.suptitle(u'Original image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "OCoFwCotfQ-Z"
   },
   "outputs": [],
   "source": [
    "# compare it with a local mean\n",
    "mvalab.visusar(np.sqrt(ima_int_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLu7SNt72yrI"
   },
   "source": [
    "## Question 5.\n",
    "Comment the result and compare with a local mean. \n",
    "\n",
    "---\n",
    "__Réponse :__\n",
    "\n",
    "Avec le filtre de Lee, l'image est moyennée, mais il reste du bruit, là ou l'images était hétérogène. Dans ce cas on garde les valeurs bruitées du début, plutôt que de moyenner\n",
    "\n",
    "On utilise le coefficient de cariation pour vérifier si la zone est homogène avant de moyenner. Si la zone n'est pas homogène, on n'effectue pas le moyennage\n",
    "\n",
    "Par rapport à la moyenne locale, qui est un peu débruitée mais floue, le filtre de Lee est meilleur, car il préserve mieux les bords et les points brillants. Ces points sont en effets préservés grâce à ks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "GW91lcjJfQ-f"
   },
   "source": [
    "### Filtering of image \"Lely\" and comparison with a deep learning algorithm\n",
    "Repeat the process done above to denoise a crop of image \"Lely\" using the Lee filter. Then, compare it with the result of a deep learning algorithm called SAR-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "T3KenFQS86xS"
   },
   "outputs": [],
   "source": [
    "# REPEAT THE PROCESS WITH THE CROP FROM LELY\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "x = 470\n",
    "y = 2230\n",
    "pat_size = 512\n",
    "lely_crop_slc = im_slc_senti_lely[x : x+pat_size, y : y+pat_size]\n",
    "mvalab.visusar(np.abs(lely_crop_slc))\n",
    "\n",
    "# take the intensity \n",
    "ima_int = np.multiply(lely_crop_slc, np.conj(lely_crop_slc))\n",
    "mvalab.visusar(np.sqrt(ima_int));\n",
    "\n",
    "# create the average window\n",
    "size_window = 7\n",
    "masque_loc = np.ones((size_window,size_window))/(size_window*size_window)\n",
    "\n",
    "# compute the mean image\n",
    "ima_int_mean = signal.convolve2d(ima_int,masque_loc,mode='same')\n",
    "\n",
    "# compute the variance image (var{I} = E{I^2} - E{I}^2)\n",
    "ima_int_square = np.multiply(ima_int,ima_int) # I^2 \n",
    "ima_int_mean_square =  signal.convolve2d(ima_int_square,masque_loc,mode='same') # E{I^2}\n",
    "ima_variance = ima_int_mean_square-np.multiply(ima_int_mean,ima_int_mean) \n",
    "\n",
    "# compute coefficient of variation\n",
    "ima_coeff_var = np.divide(np.sqrt(ima_variance),ima_int_mean)\n",
    "mvalab.visusar(ima_coeff_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "RtCnfAit86xU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute ks, by taking ima_coeff_var previously computed\n",
    "ks = 1-(1/ima_coeff_var)\n",
    "\n",
    "# force k to have values comprised in the range [0,1]\n",
    "ks[ks<0]=0\n",
    "mvalab.visusar(ks)\n",
    "\n",
    "# filter the image\n",
    "image_lee_filtered = ima_int_mean+np.multiply(ks,ima_int-ima_int_mean)\n",
    "mvalab.visusar(np.sqrt(image_lee_filtered))\n",
    "mvalab.visusar(np.sqrt(ima_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "vWs2xzZr86xW"
   },
   "source": [
    "### Denoised image: SAR-CNN\n",
    "The Lee filter presents some limits. More recent approaches to suppress noise rely on sofisticated algorithms. You can plot the image of Lely denoised using a deep learning algorthm called SAR-CNN and compare visually the result with the image filtered using the Lee filter.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "B9MirDyQ86xX"
   },
   "outputs": [],
   "source": [
    "image = 'Lely_denoised.IMF'\n",
    "pageweb = 'https://perso.telecom-paristech.fr/dalsasso/TPSAR/pilelely/denoised_SARCNN/'\n",
    "im_lely_multitemp_denoised = mvalab.imz2mat(pageweb+image)\n",
    "mvalab.visusar(im_lely_multitemp_denoised[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9niNonR94LxS"
   },
   "source": [
    "## Question 7.\n",
    "Do a comparison between the CNN filetring and the Lee filter. What is the problem for CNN methods for SAR images ? \n",
    "\n",
    "---\n",
    "\n",
    "__Réponse :__\n",
    "\n",
    "Le filtre de Lee garde du bruit, même dans les zones homogènes, et encore plus dans les zones avec beaucoup de points lumineux\n",
    "\n",
    "Au contraire le filtre CNN lisse trop les zones homogènes, commes les champs, ce qui rend l'image trop floue. Par contre le filtre CNN filtre bien les zones avec beaucoup de rétrodiffuseurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "m9Fq7oTlfQ-v"
   },
   "source": [
    "## F. Method noise\n",
    "Performances of a denoising algorithm can be visually interpreted by looking at the *residual noise* (i.e. the ratio between the noisy image and the denoised image, in intensity). For a quantitative evaluation, we can look at the noise statistics, knowing that, in intensity, statistics of speckle S are:\n",
    "- $\\mu_S=1$\n",
    "- $\\sigma^2_S = \\frac{1}{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "EWUW97aZfQ-v"
   },
   "outputs": [],
   "source": [
    "# Plot the residual noise as division between ima_int and the denoised image\n",
    "# study the moments and the histogram of the residual noise\n",
    "\n",
    "res_noise_lee = ima_int/image_lee_filtered\n",
    "mvalab.visusar(res_noise_lee)\n",
    "plt.hist(res_noise_lee.flatten(),density='True')\n",
    "mean_lee = np.mean(res_noise_lee)\n",
    "var_lee = np.var(res_noise_lee)\n",
    "print('LEE FILTER: mean = '+str(mean_lee)+' and var = '+str(var_lee))\n",
    "\n",
    "# warning: im_lely_multitemp_denoised[0] is in amplitude\n",
    "res_noise_deep = ima_int/(np.abs(im_lely_multitemp_denoised[0]**2))\n",
    "mvalab.visusar(res_noise_deep)\n",
    "plt.hist(res_noise_deep.flatten(),density='True')\n",
    "mean_deep = np.mean(res_noise_deep)\n",
    "var_deep = np.var(res_noise_deep)\n",
    "print('SAR-CNN: mean = '+str(mean_deep)+' and var = '+str(var_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZvLf_6UD4fok"
   },
   "source": [
    "## Question 8. \n",
    "What is the interest of computing the method noise ? What are your conclusions for the two previous filters ?\n",
    "\n",
    "---\n",
    "__Réponse :__\n",
    "\n",
    "Cette visualisation met en avant les zones qui ont été débruitées ou non. S l'image était correctement débruitée, nous obtiendrions du bruit pur. \n",
    "\n",
    "Sur le filtre de Lee, les zones sombres correspondent aux zones non débtruitées, là ou ks était élevé. Les contours apparaissent en sombre sur l'image, car ce sont des zones de forte variance. La moyenne est inférieure à 1, car certaines zones n'ont pas été transformées, donc le filtre est biaisé.\n",
    "\n",
    "Pour la méthode de deep learning, le débruitage est plus homogène, mais on aperçoit enclore certains contours.\n",
    "La moyenne est très proche de 1, ce qui montre que le débruitage n'est pas biaisé, mais la variance est plus grande que le résultat attendu.\n",
    "\n",
    "On voit que les histogrammes ne correspondent pas exactement à la distribution de Rayleigh attendue."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tp_stats(1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
